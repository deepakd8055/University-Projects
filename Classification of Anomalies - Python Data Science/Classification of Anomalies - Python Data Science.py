# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XszfQc-ZS0d8LWGpEV23k24Bh1T9Wjbm

## Importing Libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
from IPython.display import display
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import warnings
import csv
import sys

"""## Calculating Attributes Importance using Gini Importance and Displaying Graph"""

def AttributesWeight(classifieer, data):
    feats = {}
    for feature, importance in zip(data.columns, classifieer.feature_importances_):
        feats[feature] = importance
    importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-Importance'})
    importances = importances.sort_values(by='Gini-Importance', ascending=False)
    importances = importances.reset_index()
    importances = importances.rename(columns={'index': 'Features'})
    sns.set(font_scale = 5)
    sns.set(style="whitegrid", color_codes=True, font_scale = 1.7)
    fig, ax = plt.subplots()
    fig.set_size_inches(30,15)
    sns.barplot(x=importances['Gini-Importance'], y=importances['Features'], data=importances, color='skyblue')
    plt.xlabel('Importance', fontsize=25, weight = 'bold')
    plt.ylabel('Features', fontsize=25, weight = 'bold')
    plt.title('Feature Importance', fontsize=25, weight = 'bold')
    display(plt.show())
    display(importances)
    return importances.iloc[:20,:-1].values

"""## Feature Scaling the columns for better performance"""

def FeatureScaling(x_train, x_test):
    sc = StandardScaler()
    x_train = sc.fit_transform(x_train)
    x_test = sc.transform(x_test)
    return x_train, x_test

"""## Function for Implementing Random Forest, Decison Tree and K-Neighbours classification with 5-Fold cross validation on datasets"""

def Classification(classifieer,x,y):
    dicto = {}
    accuracyAvg = {}
    for key in classifieer:
        scores = []
        avgScores = 0.0
        cv = KFold(n_splits=5, random_state=0, shuffle=True)
        for train_index, test_index in cv.split(x):
            x_train, x_test, y_train, y_test = x[train_index], x[test_index], y[train_index], y[test_index]
            x_train, x_test = FeatureScaling(x_train, x_test)
            key.fit(x_train, y_train)
            y_pred = key.predict(x_test)
            cm = confusion_matrix(y_test, y_pred)
            print(classifier_n[key])
            print("Train Accuracy: {:.2f}%".format(accuracy_score(y_train, key.predict(x_train))*100))
            print("Test Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred)*100))
            print()
            scores.append(float("{:.2f}".format(key.score(x_test, y_test)*100)))
            avgScores += scores[-1]
        accuracyAvg[classifieer[key]] = avgScores/5
        dicto[classifieer[key]] = scores
    return dicto, accuracyAvg

"""## Displaying Comparision Graph"""

def ComparisionGraph(means1, means2):
  n_groups = 3
  means_1 = []
  means_2 = []
  for key in means1:
    means_1.append(means1[key])

  for key in means2:
    means_2.append(means2[key])
  means_allVariables = (means_1[0], means_1[1], means_1[2])
  means_20Variables = (means_2[0], means_2[1], means_2[2])

  # create plot

  fig, ax = plt.subplots(figsize=(8,10))
  ax.set_ylim([85, 100])
  index = np.arange(n_groups)
  bar_width = 0.25
  opacity = 0.8

  rects1 = plt.bar(index, means_allVariables, bar_width,
  alpha=opacity,
  color='b',
  label='All variables')

  rects2 = plt.bar(index + bar_width, means_20Variables, bar_width,
  alpha=opacity,
  color='g',
  label='20 variables')

  
  plt.xlabel('Classification Algorithms')
  plt.ylabel('Accuracy')
  plt.title('Mean Accuracy Scores')
  plt.xticks(index + bar_width, ('RandomForest', 'K-Neighbours', 'DecisionTree'))
  plt.legend()

  plt.tight_layout()
  plt.show()

"""## Importing Datasets"""

df = []
warnings.filterwarnings('ignore')
for i in range(0,3):
    fileName = 'data'+str(i+1)+'.csv'
    ds = pd.read_csv(fileName)
    df.append(ds)
ds = pd.concat(df, axis=0, ignore_index=True)

x = ds.iloc[:,:-1].values
y = ds.iloc[:,-1].values

print(type(x))

with open('data1.csv',mode='r') as csvFile:
    read = csv.reader(csvFile, delimiter = ',')
    lis = list()
    for row in read:
        for index in range(0,len(row)):
            lis.append(row[index]) #taking variables from csv to list
        break

"""## Checking for NAN's and Infinite Values and fixing"""

for i in range(0,len(x)):
    for j in range(0,len(x[i])):
        if x[i][j]>= np.inf:
            x[i][j] = sys.float_info.max - 1
        elif x[i][j]<= -np.inf:
            x[i][j] = sys.float_info.min + 1

x.astype(np.float32)
x = np.nan_to_num(x.astype(np.float32))


#Converting categorical target data to numerical
le = LabelEncoder()
y = le.fit_transform(y)

"""## Implementing Classification on Datasets"""

#classifier3 = LogisticRegression()
classifier1 = RandomForestClassifier(n_estimators = 500, bootstrap = True, n_jobs = -1, max_depth = 100)
classifier2 = KNeighborsClassifier(n_neighbors = 2)
classifier3 = DecisionTreeClassifier(criterion = 'entropy')

classifier_n = {}
classifier_n[classifier1] = "Random Forest Classification"
classifier_n[classifier2] = "K-Neighbours Classification"
classifier_n[classifier3] = "Decision Tree Classification"


scores, accuracyAvg = Classification(classifier_n,x,y)
for key in scores:
    print("Accuracy of ",key,scores[key])
print()
for key in accuracyAvg:
    print("Mean Accuracy rate of ",key,": {:.2f}%".format(accuracyAvg[key]))

    
    
feat = AttributesWeight(classifier1, ds)

"""## Implementing Classification after taking top 20 features having high Gini Importance"""

indexAttr = []
print(feat)
for elementIndex in range(0, len(feat)):
    indexAttr.append(lis.index(feat[elementIndex][-1]))
print(indexAttr)
nIndex = []
for i in range(0,len(lis)):
  if i not in indexAttr:
    nIndex.append(i)

ds = ds.drop(ds.columns[nIndex], axis=1)
print(ds) 
x = ds.iloc[:,:].values



scores1, accuracyAvg1 = Classification(classifier_n,x,y)
for key in scores1:
    print("Accuracy of ",key,scores1[key])
print()
for key in accuracyAvg1:
    print("Mean Accuracy rate of ",key,": {:.2f}%".format(accuracyAvg1[key]))

feat1 = AttributesWeight(classifier1, ds)

ComparisionGraph(accuracyAvg, accuracyAvg1)
